# =============================================================================
# LM Configuration - Environment Variables
# =============================================================================
# Copy this file to .env and configure your preferred provider
# Never commit .env to version control!

# -----------------------------------------------------------------------------
# Provider Selection (REQUIRED)
# -----------------------------------------------------------------------------
# Choose one: openai, anthropic, cohere, ollama
LM_PROVIDER=openai

# =============================================================================
# OpenAI Configuration
# =============================================================================
# Use these settings when LM_PROVIDER=openai

# Required: Your OpenAI API key (starts with sk-)
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-api-key-here

# Optional: Model to use (default: gpt-3.5-turbo)
# Options: gpt-3.5-turbo, gpt-4, gpt-4-turbo-preview, etc.
OPENAI_MODEL=gpt-3.5-turbo

# Optional: Custom API base URL (default: https://api.openai.com/v1)
# Useful for Azure OpenAI or proxy servers
# OPENAI_BASE_URL=https://api.openai.com/v1

# Optional: Organization ID for API usage tracking
# OPENAI_ORG_ID=org-your-org-id

# =============================================================================
# Anthropic Configuration
# =============================================================================
# Use these settings when LM_PROVIDER=anthropic

# Required: Your Anthropic API key (starts with sk-ant-)
# Get your key from: https://console.anthropic.com/
# ANTHROPIC_API_KEY=sk-ant-your-api-key-here

# Optional: Model to use (default: claude-3-haiku-20240307)
# Options: claude-3-haiku-20240307, claude-3-sonnet-20240229, claude-3-opus-20240229
# ANTHROPIC_MODEL=claude-3-haiku-20240307

# Optional: API version (default: 2023-06-01)
# ANTHROPIC_VERSION=2023-06-01

# =============================================================================
# Cohere Configuration
# =============================================================================
# Use these settings when LM_PROVIDER=cohere

# Required: Your Cohere API key
# Get your key from: https://dashboard.cohere.com/api-keys
# COHERE_API_KEY=your-cohere-api-key-here

# Optional: Model to use (default: command)
# Options: command, command-light, command-nightly, etc.
# COHERE_MODEL=command

# =============================================================================
# Ollama Configuration (Local)
# =============================================================================
# Use these settings when LM_PROVIDER=ollama
# Ollama runs locally and does not require an API key

# Optional: Ollama server URL (default: http://localhost:11434)
# Change this if running Ollama on a different host/port
# OLLAMA_BASE_URL=http://localhost:11434

# Optional: Model to use (default: llama2)
# Options: llama2, mistral, codellama, phi, etc.
# Must be pulled first: ollama pull <model-name>
# OLLAMA_MODEL=llama2

# =============================================================================
# Usage Examples
# =============================================================================

# Example 1: Use OpenAI GPT-4
# LM_PROVIDER=openai
# OPENAI_API_KEY=sk-your-key
# OPENAI_MODEL=gpt-4

# Example 2: Use Anthropic Claude 3 Sonnet
# LM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-your-key
# ANTHROPIC_MODEL=claude-3-sonnet-20240229

# Example 3: Use Cohere Command
# LM_PROVIDER=cohere
# COHERE_API_KEY=your-key
# COHERE_MODEL=command

# Example 4: Use Local Ollama with Mistral
# LM_PROVIDER=ollama
# OLLAMA_MODEL=mistral
# OLLAMA_BASE_URL=http://localhost:11434

# =============================================================================
# Quick Start
# =============================================================================
# 1. Copy this file: cp .env.example .env
# 2. Choose a provider and uncomment its section
# 3. Add your API key (if required)
# 4. Run: cargo run
#
# For Ollama (no API key required):
# 1. Install: curl https://ollama.ai/install.sh | sh
# 2. Pull a model: ollama pull llama2
# 3. Set LM_PROVIDER=ollama in .env
# 4. Run: cargo run

# =============================================================================
# Security Notes
# =============================================================================
# - NEVER commit .env to version control
# - Keep API keys secure and rotate them regularly
# - Use different keys for development and production
# - Monitor API usage to detect unauthorized access
# - Consider using a secrets manager for production deployments
