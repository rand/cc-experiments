# BootstrapFewShot Optimization Example

A complete implementation of DSPy's BootstrapFewShot teleprompter from Rust, demonstrating how to run few-shot learning optimization workflows with progress tracking, model persistence, and comprehensive evaluation.

## Overview

This example shows how to:
- Load and prepare training datasets for optimization
- Run BootstrapFewShot teleprompter from Rust with PyO3
- Track optimization progress with structured callbacks
- Save and version compiled models with metadata
- Evaluate model performance before and after optimization
- Implement proper error handling and resource management

## What is BootstrapFewShot?

BootstrapFewShot is a DSPy teleprompter that automatically generates few-shot examples to improve module performance. It works by:

1. **Bootstrapping**: Running your module on training examples to generate intermediate outputs
2. **Selection**: Choosing the best examples based on a metric function
3. **Compilation**: Creating an optimized module with selected demonstrations embedded

### Theory

**Few-shot learning** teaches models by example. Instead of fine-tuning weights, few-shot learning provides exemplar input-output pairs in the prompt.

**Key Concepts**:
- **Bootstrapped Demonstrations**: Examples generated by the module itself on training data
- **Labeled Demonstrations**: Human-provided ground truth examples
- **Metric Function**: Evaluates quality of outputs (accuracy, F1, custom metrics)
- **Max Demos**: Limits number of examples to include (controls prompt length)

**Algorithm**:
```
1. For each training example:
   a. Run module to generate prediction
   b. Score prediction using metric function
   c. If score passes threshold, save as demonstration
2. Select top K demonstrations by score
3. Compile new module with demonstrations embedded
4. Return optimized module
```

**Trade-offs**:
- **Pros**: No fine-tuning, fast, works with any LLM, interpretable
- **Cons**: Increases prompt length, costs per call, limited by context window

## Prerequisites

- Rust 1.70+ with Cargo
- Python 3.8+ with DSPy installed (`pip install dspy-ai`)
- OpenAI API key or other LLM provider configured

## Installation

```bash
# Install dependencies
cargo build

# Set API key
export OPENAI_API_KEY="your-key-here"

# Run example
cargo run
```

## Project Structure

```
bootstrap-fewshot/
├── Cargo.toml              # Dependencies and project config
├── README.md               # This file
├── src/
│   ├── lib.rs             # Core optimization logic (400-500 lines)
│   └── main.rs            # CLI and workflow orchestration (300-400 lines)
└── data/
    └── trainset.json      # Sample training data
```

## Usage

### Basic Optimization

```bash
# Run with defaults
cargo run

# With custom training data
cargo run -- --train-data ./my_data.json

# With custom hyperparameters
cargo run -- --max-bootstrapped 8 --max-labeled 4
```

### As a Library

```rust
use bootstrap_fewshot::{
    TrainingExample, OptimizationConfig, OptimizationResult,
    run_bootstrap_fewshot, save_compiled_model, evaluate_model
};
use pyo3::prelude::*;

fn main() -> anyhow::Result<()> {
    // Load training data
    let trainset = vec![
        TrainingExample {
            question: "What is the capital of France?".to_string(),
            answer: "Paris".to_string(),
        },
        // ... more examples
    ];

    Python::with_gil(|py| {
        // Create QA module
        let module = create_qa_module(py)?;

        // Configure optimization
        let config = OptimizationConfig {
            max_bootstrapped_demos: 4,
            max_labeled_demos: 8,
            metric_threshold: 0.7,
            verbose: true,
        };

        // Run optimization
        let result = run_bootstrap_fewshot(
            py,
            module.as_ref(py),
            trainset,
            config,
        )?;

        println!("Optimization complete!");
        println!("  Score: {:.2}", result.optimization_score);
        println!("  Examples used: {}", result.num_examples);

        // Save model
        save_compiled_model(
            py,
            result.compiled_model.as_ref(py),
            "./output/model",
        )?;

        Ok(())
    })
}
```

## Example Output

```
Starting BootstrapFewShot optimization...
Configured DSPy with gpt-3.5-turbo
Training examples: 50

[Bootstrap Progress]
  Step 1/50: Processing example (score: 0.85)
  Step 2/50: Processing example (score: 0.92)
  ...
  Step 50/50: Processing example (score: 0.88)

Selection phase...
  Selected 4 bootstrapped demonstrations
  Selected 8 labeled demonstrations

Compiling optimized module...

Optimization complete!
  Examples used: 50
  Demonstrations selected: 12
  Optimization score: 0.89

Evaluating baseline model...
  Accuracy: 68.5%
  Average latency: 234ms

Evaluating optimized model...
  Accuracy: 89.2%
  Average latency: 287ms

Improvement: +20.7% accuracy

Model saved to: ./output/model.json
Metadata saved to: ./output/metadata.json
```

## Configuration Options

### OptimizationConfig

```rust
pub struct OptimizationConfig {
    /// Maximum bootstrapped demonstrations to select
    pub max_bootstrapped_demos: usize,  // Default: 4

    /// Maximum labeled demonstrations to include
    pub max_labeled_demos: usize,       // Default: 8

    /// Minimum metric score for demonstration selection
    pub metric_threshold: f64,          // Default: 0.7

    /// Enable verbose logging during optimization
    pub verbose: bool,                  // Default: true

    /// Random seed for reproducibility
    pub random_seed: Option<u64>,       // Default: None
}
```

### Training Data Format

JSON array of question-answer pairs:

```json
[
  {
    "question": "What is the capital of France?",
    "answer": "Paris"
  },
  {
    "question": "Who wrote Romeo and Juliet?",
    "answer": "William Shakespeare"
  }
]
```

## Key Components

### 1. TrainingExample

Represents a single training data point:

```rust
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TrainingExample {
    pub question: String,
    pub answer: String,
}
```

### 2. OptimizationResult

Contains results of optimization process:

```rust
pub struct OptimizationResult {
    pub compiled_model: Py<PyAny>,
    pub num_examples: usize,
    pub num_demonstrations: usize,
    pub optimization_score: f64,
    pub duration_secs: f64,
}
```

### 3. Progress Tracking

Callback-based progress monitoring:

```rust
pub type ProgressCallback = Box<dyn Fn(usize, usize, f64) + Send>;

// Usage
run_bootstrap_fewshot_with_progress(
    py,
    module,
    trainset,
    config,
    Box::new(|step, total, score| {
        println!("[{}/{}] Score: {:.2}", step, total, score);
    }),
)?;
```

### 4. Metric Functions

Custom evaluation metrics:

```rust
pub fn create_accuracy_metric(py: Python) -> PyResult<Py<PyAny>> {
    // Creates Python function: lambda pred, label: pred.lower() == label.lower()
}

pub fn create_f1_metric(py: Python) -> PyResult<Py<PyAny>> {
    // Creates F1 score metric for more nuanced evaluation
}
```

## Best Practices

### DO

- **Validate training data** format before optimization
- **Start small** with 10-20 examples to test workflow
- **Monitor progress** using callbacks for long-running optimizations
- **Evaluate before/after** to quantify improvement
- **Version models** with metadata (hyperparameters, scores, timestamps)
- **Set timeouts** for production workflows
- **Use appropriate metrics** that match your task goals

### DON'T

- **Optimize without validation** set (leads to overfitting)
- **Use too many demonstrations** (increases latency and cost)
- **Ignore metric selection** (wrong metric = wrong optimization)
- **Skip baseline evaluation** (can't measure improvement)
- **Forget error handling** (Python exceptions need proper wrapping)

## Troubleshooting

### Issue: Low Optimization Score

**Symptom**: Final score barely improves from baseline

**Causes**:
- Training data too small (need 50+ examples)
- Metric function not well-aligned with task
- Max demos set too low
- Training data quality issues

**Solutions**:
1. Increase training set size
2. Verify metric function correctness
3. Increase `max_bootstrapped_demos` to 8-16
4. Validate training data quality

### Issue: Optimization Takes Too Long

**Symptom**: Process runs for hours without completion

**Causes**:
- Too many training examples
- Slow LLM responses
- No timeout configured

**Solutions**:
1. Start with subset of data (50-100 examples)
2. Use faster base model (gpt-3.5-turbo vs gpt-4)
3. Implement timeout logic
4. Run optimization asynchronously

### Issue: Model Fails to Load

**Symptom**: `Failed to load compiled model`

**Causes**:
- File corruption during save
- Version mismatch between DSPy versions
- Missing metadata

**Solutions**:
1. Verify file permissions and integrity
2. Check DSPy version compatibility
3. Ensure metadata.json exists alongside model.json

### Issue: Python Import Errors

**Symptom**: `ModuleNotFoundError: No module named 'dspy'`

**Causes**:
- DSPy not installed in Python environment
- Using wrong Python interpreter

**Solutions**:
```bash
# Install DSPy
pip install dspy-ai

# Verify installation
python -c "import dspy; print(dspy.__version__)"

# If using venv, activate it first
source venv/bin/activate
```

## Performance Considerations

### Optimization Time

- **10 examples**: ~30 seconds
- **50 examples**: ~2-3 minutes
- **100 examples**: ~5-10 minutes
- **500 examples**: ~30-60 minutes

Times vary based on LLM speed and complexity.

### Memory Usage

- Minimal: ~50-100MB base
- Scales with: training set size and model state
- Peak during: compilation phase

### Cost Estimation

**Per optimization run**:
- Training examples: N
- LLM calls: ~N * 2 (bootstrap + evaluation)
- Tokens per call: ~500-1000
- Cost with gpt-3.5-turbo: ~$0.01-0.05 per 50 examples

## Advanced Usage

### Custom Metric Functions

```rust
// Example: Semantic similarity metric
pub fn create_semantic_similarity_metric(
    py: Python,
    threshold: f64,
) -> PyResult<Py<PyAny>> {
    let code = format!(r#"
def metric(prediction, label):
    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer('all-MiniLM-L6-v2')
    pred_emb = model.encode(prediction)
    label_emb = model.encode(label)
    similarity = cosine_similarity(pred_emb, label_emb)
    return similarity > {}
"#, threshold);

    py.run(&code, None, None)?;
    let locals = py.eval("locals()", None, None)?;
    let metric = locals.get_item("metric")?;
    Ok(metric.into())
}
```

### Batch Evaluation

```rust
// Evaluate multiple models in parallel
use tokio::task;

let handles: Vec<_> = model_paths
    .iter()
    .map(|path| {
        let path = path.clone();
        let test_set = test_set.clone();
        task::spawn(async move {
            evaluate_model(&path, test_set).await
        })
    })
    .collect();

let results = futures::future::join_all(handles).await;
```

### Progressive Optimization

```rust
// Start with small demos, gradually increase
for max_demos in [2, 4, 8, 16] {
    let config = OptimizationConfig {
        max_bootstrapped_demos: max_demos,
        ..Default::default()
    };

    let result = run_bootstrap_fewshot(py, module, trainset.clone(), config)?;

    println!("Max demos {}: score {:.2}", max_demos, result.optimization_score);
}
```

## Related Examples

- **miprov2-optimization**: Advanced multi-stage optimization
- **model-versioning**: Semantic versioning for compiled models
- **evaluation-harness**: Comprehensive evaluation workflows
- **ab-testing**: Production A/B testing framework

## References

- [DSPy Documentation](https://dspy-docs.vercel.app/)
- [BootstrapFewShot Paper](https://arxiv.org/abs/2310.03714)
- [Few-Shot Learning Survey](https://arxiv.org/abs/2101.00190)
- [PyO3 User Guide](https://pyo3.rs/)

## License

MIT License - See project root for details.

---

**Version**: 0.1.0
**Last Updated**: 2025-10-30
**Maintainer**: PyO3-DSPy Integration Team
