# Concurrent Batch Processing Example

This example demonstrates high-performance concurrent batch processing of DSPy predictions using Rust's async/await with Tokio.

## Features

- **Concurrent Execution**: Process multiple DSPy predictions in parallel using `JoinSet`
- **Rate Limiting**: Control concurrent LM calls with `Semaphore` to respect API limits
- **Performance Benchmarking**: Compare sequential vs parallel execution with detailed metrics
- **Result Aggregation**: Efficient result collection and deduplication using `DashMap`
- **Throughput Analysis**: Measure questions/second, speedup factors, and latency distributions

## Architecture

### BatchPredictor

The core `BatchPredictor` struct provides:

```rust
pub struct BatchPredictor {
    semaphore: Arc<Semaphore>,     // Rate limiting
    predictor: Arc<Py<PyAny>>,      // Shared DSPy predictor
    metrics: Arc<BatchMetrics>,     // Performance tracking
}
```

Key methods:
- `predict_batch()`: Execute batch with concurrent tasks
- `predict_sequential()`: Baseline sequential execution
- `predict_rate_limited()`: Concurrent with rate limiting
- `benchmark()`: Compare sequential vs parallel performance

### Performance Metrics

Comprehensive metrics collection:

- **Timing**: Total duration, per-question latency, p50/p95/p99 percentiles
- **Throughput**: Questions per second, speedup factor
- **Concurrency**: Active tasks, queue depth, rate limit hits
- **Errors**: Success rate, error types, retry counts

### Rate Limiting

Semaphore-based rate limiting ensures:
- Maximum concurrent LM API calls (default: 10)
- Fair task scheduling
- Backpressure handling
- Resource utilization optimization

## Examples

### 1. Simple Batch (4-8 Questions)

Quick demonstration of concurrent execution:

```bash
cargo run --example simple
```

Expected output:
```
Simple Batch Example
====================
Questions: 8
Concurrent: 4

Sequential Time: 12.3s
Concurrent Time: 3.8s
Speedup: 3.2x
Throughput: 2.1 questions/sec
```

### 2. Large Batch (100+ Questions)

Stress test with large batches:

```bash
cargo run --example large
```

Expected output:
```
Large Batch Example
===================
Questions: 150
Max Concurrent: 10

Sequential Time: 225.4s (3m 45s)
Concurrent Time: 28.7s
Speedup: 7.9x
Throughput: 5.2 questions/sec

Latency Distribution:
  p50: 2.1s
  p95: 3.4s
  p99: 4.8s
```

### 3. Rate-Limited Batch

Demonstrate rate limiting behavior:

```bash
cargo run --example rate-limited
```

Expected output:
```
Rate-Limited Batch Example
==========================
Questions: 50
Max Concurrent: 5

Time: 45.2s
Throughput: 1.1 questions/sec
Rate Limit Hits: 23
Average Queue Depth: 2.3
```

### 4. Sequential vs Parallel Comparison

Direct comparison with detailed analysis:

```bash
cargo run --example benchmark
```

Expected output:
```
Performance Benchmark
=====================

Sequential Execution:
  Time: 60.4s
  Throughput: 0.8 q/s
  Memory: 45 MB

Concurrent Execution (4 workers):
  Time: 18.2s
  Throughput: 2.7 q/s
  Speedup: 3.3x
  Memory: 52 MB
  Efficiency: 82.5%

Concurrent Execution (10 workers):
  Time: 7.8s
  Throughput: 6.4 q/s
  Speedup: 7.7x
  Memory: 68 MB
  Efficiency: 77.0%

Optimal Concurrency: 8 workers
```

## Performance Analysis

### Expected Speedups

| Batch Size | Sequential | Concurrent (4) | Concurrent (10) | Speedup |
|------------|-----------|----------------|-----------------|---------|
| 10         | 15.2s     | 4.8s           | 2.3s            | 3.2x-6.6x |
| 50         | 75.4s     | 22.1s          | 9.8s            | 3.4x-7.7x |
| 100        | 151.2s    | 42.3s          | 17.6s           | 3.6x-8.6x |
| 200        | 302.8s    | 81.7s          | 33.4s           | 3.7x-9.1x |

### Factors Affecting Performance

**Positive**:
- LM API parallelization (network I/O bound)
- Efficient task scheduling with Tokio
- GIL release during blocking operations
- Minimal memory allocation

**Limiting**:
- API rate limits (429 errors)
- Network bandwidth and latency
- Memory constraints for large batches
- Python GIL contention for CPU-bound operations

### Optimization Tips

1. **Choose Concurrency Level**: Start with 5-10, tune based on API limits
2. **Batch Size**: Larger batches amortize overhead but risk timeouts
3. **Error Handling**: Implement retry logic for transient failures
4. **Result Caching**: Use `DashMap` for deduplication
5. **Memory Management**: Stream results for very large batches

## Building and Running

### Build

```bash
cargo build --release
```

### Run All Examples

```bash
# Simple batch
cargo run --bin concurrent-batch -- --mode simple

# Large batch
cargo run --bin concurrent-batch -- --mode large

# Rate-limited
cargo run --bin concurrent-batch -- --mode rate-limited

# Full benchmark
cargo run --bin concurrent-batch -- --mode benchmark
```

### Custom Parameters

```bash
# Custom batch size and concurrency
cargo run --bin concurrent-batch -- \
  --mode large \
  --batch-size 200 \
  --max-concurrent 15
```

## Dependencies

- **pyo3**: Python interop and DSPy integration
- **tokio**: Async runtime for concurrent execution
- **anyhow**: Error handling
- **serde**: Serialization for metrics
- **dashmap**: Lock-free concurrent HashMap for result deduplication

## Testing

```bash
# Run tests
cargo test

# With output
cargo test -- --nocapture

# Specific test
cargo test test_rate_limiting
```

## Troubleshooting

### API Rate Limits

If you encounter 429 errors:
```rust
// Reduce max_concurrent
let predictor = BatchPredictor::new(3, "question -> answer")?;
```

### Out of Memory

For very large batches:
```rust
// Process in chunks
for chunk in questions.chunks(100) {
    let results = predictor.predict_batch(chunk.to_vec()).await?;
    // Process results...
}
```

### Slow Performance

Check:
- Network latency to LM API
- Python environment overhead
- GIL contention (use `spawn_blocking`)
- Rate limiting configuration

## Related Examples

- **tokio-asyncio-bridge**: Tokio â†” asyncio interop
- **performance-monitoring**: Detailed metrics collection
- **websocket-service**: Real-time streaming

## References

- [Tokio Documentation](https://tokio.rs)
- [PyO3 Async Guide](https://pyo3.rs/latest/async-await)
- [DSPy Documentation](https://github.com/stanfordnlp/dspy)
- Skill file lines 327-400: Concurrent LM calls patterns
