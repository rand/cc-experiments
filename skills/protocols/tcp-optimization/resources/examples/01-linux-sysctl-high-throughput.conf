# Linux sysctl configuration for high-throughput TCP optimization
# Optimized for 10+ Gbps networks with moderate latency (10-50ms RTT)
#
# Usage:
#   1. Copy to /etc/sysctl.d/99-tcp-high-throughput.conf
#   2. Apply: sudo sysctl -p /etc/sysctl.d/99-tcp-high-throughput.conf
#   3. Verify: sysctl net.ipv4.tcp_congestion_control
#
# Tested on:
#   - Ubuntu 22.04 LTS
#   - Debian 12
#   - RHEL 9 / Rocky Linux 9
#   - Kernel 5.10+

# ============================================================================
# TCP Buffer Sizes
# ============================================================================
# Calculated for 10 Gbps × 50ms RTT = 62.5 MB BDP
# Using 2× BDP for safety = 125 MB

# TCP read buffer: min, default, max (bytes)
net.ipv4.tcp_rmem = 4096 131072 134217728

# TCP write buffer: min, default, max (bytes)
net.ipv4.tcp_wmem = 4096 131072 134217728

# Maximum socket receive buffer (must be >= tcp_rmem max)
net.core.rmem_max = 134217728

# Maximum socket send buffer (must be >= tcp_wmem max)
net.core.wmem_max = 134217728

# Default socket receive buffer
net.core.rmem_default = 131072

# Default socket send buffer
net.core.wmem_default = 131072

# Maximum ancillary buffer size
net.core.optmem_max = 65536

# ============================================================================
# TCP Memory Limits
# ============================================================================
# Total TCP memory allocation (pages of 4KB)
# min: 4 GB, pressure: 8 GB, max: 16 GB
net.ipv4.tcp_mem = 1048576 2097152 4194304

# ============================================================================
# Congestion Control
# ============================================================================
# BBR (Bottleneck Bandwidth and RTT) congestion control
# Superior performance on high BDP networks
net.ipv4.tcp_congestion_control = bbr

# Fair Queueing scheduler (required for BBR)
net.core.default_qdisc = fq

# ============================================================================
# TCP Window Scaling
# ============================================================================
# Enable window scaling (RFC 1323)
# Allows windows > 64KB (essential for high throughput)
net.ipv4.tcp_window_scaling = 1

# ============================================================================
# TCP Timestamps
# ============================================================================
# Enable TCP timestamps (RFC 1323)
# Required for accurate RTT measurement and PAWS
net.ipv4.tcp_timestamps = 1

# ============================================================================
# Selective Acknowledgment (SACK)
# ============================================================================
# Enable SACK (RFC 2018)
# Efficient recovery from packet loss
net.ipv4.tcp_sack = 1

# Enable duplicate SACK (RFC 2883)
# Helps detect spurious retransmissions
net.ipv4.tcp_dsack = 1

# Enable forward acknowledgment
# Uses SACK info for congestion control
net.ipv4.tcp_fack = 1

# ============================================================================
# TCP Performance Tuning
# ============================================================================
# Don't reset congestion window after idle period
# Maintains performance for bursty traffic
net.ipv4.tcp_slow_start_after_idle = 0

# Enable MTU probing (RFC 4821)
# Discovers optimal MTU along path
net.ipv4.tcp_mtu_probing = 1

# Enable auto-tuning of receive buffer
net.ipv4.tcp_moderate_rcvbuf = 1

# Don't cache TCP metrics
# Use fresh measurements each time
net.ipv4.tcp_no_metrics_save = 1

# ============================================================================
# TCP Fast Open
# ============================================================================
# Enable TCP Fast Open for client and server (RFC 7413)
# Reduces latency for short flows
# 0: disabled, 1: client, 2: server, 3: both
net.ipv4.tcp_fastopen = 3

# ============================================================================
# Connection Management
# ============================================================================
# Reuse TIME_WAIT sockets for new connections
# Safe for client connections
net.ipv4.tcp_tw_reuse = 1

# Maximum number of TIME_WAIT sockets
# Increase for high connection rate
net.ipv4.tcp_max_tw_buckets = 2000000

# Time to hold socket in FIN-WAIT-2 state (seconds)
# Reduce to cleanup faster
net.ipv4.tcp_fin_timeout = 30

# ============================================================================
# SYN Queue
# ============================================================================
# Maximum SYN backlog
# Increase for high connection rate
net.ipv4.tcp_max_syn_backlog = 8192

# Enable SYN cookies for SYN flood protection
net.ipv4.tcp_syncookies = 1

# ============================================================================
# Keep-Alive
# ============================================================================
# Time before sending keep-alive probes (seconds)
net.ipv4.tcp_keepalive_time = 600

# Interval between keep-alive probes (seconds)
net.ipv4.tcp_keepalive_intvl = 60

# Number of keep-alive probes before giving up
net.ipv4.tcp_keepalive_probes = 3

# ============================================================================
# Retransmission
# ============================================================================
# Number of SYN retransmits
net.ipv4.tcp_syn_retries = 5

# Number of SYN-ACK retransmits
net.ipv4.tcp_synack_retries = 5

# Threshold for fast retransmit
net.ipv4.tcp_retries1 = 3

# Maximum retransmit attempts
net.ipv4.tcp_retries2 = 15

# ============================================================================
# IP Settings
# ============================================================================
# Local port range for outbound connections
net.ipv4.ip_local_port_range = 10000 65535

# ============================================================================
# Core Network Settings
# ============================================================================
# Maximum socket listen() backlog
net.core.somaxconn = 4096

# Maximum packets in device queue
net.core.netdev_max_backlog = 5000

# Maximum number of packets processed per NAPI poll
net.core.netdev_budget = 300

# ============================================================================
# Notes
# ============================================================================
# After applying this configuration:
#
# 1. Enable hardware offloading (if not already enabled):
#    sudo ethtool -K eth0 tso on gso on gro on
#
# 2. Set RSS queues to match CPU count:
#    sudo ethtool -L eth0 combined $(nproc)
#
# 3. Set initial congestion window on default route:
#    sudo ip route change default via <gateway> dev eth0 initcwnd 10
#
# 4. Enable jumbo frames if supported:
#    sudo ip link set eth0 mtu 9000
#
# 5. Test throughput with iperf3:
#    iperf3 -c <server> -t 60 -P 4
#
# 6. Monitor TCP statistics:
#    ss -tin
#    nstat -az | grep Tcp
